# =======================================================================
# Platform Name            platys-platform
# Platform Stack:          trivadis/platys-modern-data-platform
# Platform Stack Version:  1.15.0-preview
# =======================================================================
version: '3.5'
networks:
  default:
    name: platys-platform
# backward compatiblity to platform < 1.14.0
# Enable PostgreSQL or MySQL for MLflow server
services:
  #  ================================== Apache Spark 2.x ========================================== #
  spark-master:
    image: trivadis/apache-spark-master:3.2.1-hadoop3.2
    container_name: spark-master
    hostname: spark-master
    labels:
      com.platys.name: spark
      com.platys.webui.title: Spark UI
      com.platys.webui.url: http://${PUBLIC_IP}:8080
    ports:
      - 6066:6066
      - 7077:7077
      - 8080:8080
      - 4040-4044:4040-4044
    env_file:
      - ./conf/hadoop.env
    environment:
      CORE_CONF_fs_s3a_endpoint: http://minio-1:9000
      CORE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_endpoint: http://minio-1:9000
      HIVE_SITE_CONF_fs_s3a_access_key: V42FCGRVMK24JJ8DHUYG
      HIVE_SITE_CONF_fs_s3a_secret_key: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio-1:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      SPARK_MASTER_OPTS: -Dspark.deploy.defaultCores=3
      INIT_DAEMON_STEP: setup_spark
      MASTER: spark://spark-master:7077
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages:
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars: /extra-jars/postgresql-42.3.4.jar
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
    restart: unless-stopped
  spark-worker-1:
    image: trivadis/apache-spark-worker:3.2.1-hadoop3.2
    container_name: spark-worker-1
    hostname: spark-worker-1
    labels:
      com.platys.name: spark
    depends_on:
      - spark-master
    ports:
      - 28111:28111
    env_file:
      - ./conf/hadoop.env
    environment:
      SPARK_MASTER: spark://spark-master:7077
      SPARK_WORKER_WEBUI_PORT: '28111'
      SPARK_WORKER_OPTS: -Dspark.worker.cleanup.enabled=true
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      CORE_CONF_fs_s3a_endpoint: http://minio-1:9000
      CORE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_endpoint: http://minio-1:9000
      HIVE_SITE_CONF_fs_s3a_access_key: V42FCGRVMK24JJ8DHUYG
      HIVE_SITE_CONF_fs_s3a_secret_key: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio-1:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages:
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars: /extra-jars/postgresql-42.3.4.jar
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
    restart: unless-stopped
  spark-worker-2:
    image: trivadis/apache-spark-worker:3.2.1-hadoop3.2
    container_name: spark-worker-2
    hostname: spark-worker-2
    labels:
      com.platys.name: spark
    depends_on:
      - spark-master
    ports:
      - 28112:28112
    env_file:
      - ./conf/hadoop.env
    environment:
      SPARK_MASTER: spark://spark-master:7077
      SPARK_WORKER_WEBUI_PORT: '28112'
      SPARK_WORKER_OPTS: -Dspark.worker.cleanup.enabled=true
      SPARK_PUBLIC_DNS: ${PUBLIC_IP}
      CORE_CONF_fs_s3a_endpoint: http://minio-1:9000
      CORE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_endpoint: http://minio-1:9000
      HIVE_SITE_CONF_fs_s3a_access_key: V42FCGRVMK24JJ8DHUYG
      HIVE_SITE_CONF_fs_s3a_secret_key: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio-1:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages:
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars: /extra-jars/postgresql-42.3.4.jar
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
    restart: unless-stopped
  #  ================================== Apache Hive Metastore ========================================== #
  hive-metastore:
    image: trivadis/apache-hive:3.1.2-postgresql-metastore-s3
    container_name: hive-metastore
    hostname: hive-metastore
    labels:
      com.platys.name: hive-metastore
    ports:
      - 9083:9083
    env_file:
      - ./conf/hadoop.env
    environment:
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      CORE_CONF_fs_s3a_endpoint: http://minio-1:9000
      CORE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_endpoint: http://minio-1:9000
      HIVE_SITE_CONF_fs_s3a_access_key: V42FCGRVMK24JJ8DHUYG
      HIVE_SITE_CONF_fs_s3a_secret_key: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      # necessary for Trino to be able to read from Avro
      HIVE_SITE_CONF_metastore_storage_schema_reader_impl: org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader
      SERVICE_PRECONDITION: hive-metastore-db:5432
    volumes:
      - ./data-transfer:/data-transfer
    command: /opt/hive/bin/hive --service metastore
    restart: unless-stopped
  hive-metastore-db:
    image: trivadis/apache-hive-metastore-postgresql:3.1.0-postgres9.5.3
    container_name: hive-metastore-db
    hostname: hive-metastore-db
    labels:
      com.platys.name: hive-metastore
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== StreamSets DataCollector ========================================== #
  streamsets-1:
    image: streamsets/datacollector:3.22.2
    container_name: streamsets-1
    hostname: streamsets-1
    labels:
      com.platys.name: streamsets
      com.platys.webui.title: StreamSets Data Collector UI
      com.platys.webui.url: http://${PUBLIC_IP}:18630
      com.platys.restapi.title: StreamSets Data Collector REST API
      com.platys.restapi.url: http://${PUBLIC_IP}:18630/collector/restapi
    ports:
      - 18630:18630
    environment:
      SDC_OFFSET_DIRECTORY: /data/custom-offset-el
      SDC_INSTALL_STAGES: streamsets-datacollector-apache-kafka_2_6-lib,streamsets-datacollector-aws-lib,streamsets-datacollector-groovy_2_4-lib,streamsets-datacollector-jdbc-lib
      SDC_INSTALL_ENTERPRISE_STAGES: ''
      SDC_JAVA_OPTS: -Xmx2g -Xms2g -Dlog4j2.formatMsgNoLookups=true
      SDC_JAVA8_OPTS: -XX:+UseG1GC
      SDC_CONF_MONITOR_MEMORY: 'true'
      SDC_CONF_PIPELINE_MAX_RUNNERS_COUNT: 50
      SDC_CONF_http_authentication: form
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/streamsets/pre-docker-entrypoint.sh:/pre-docker-entrypoint.sh
      - ./plugins/streamsets/user-libs:/opt/streamsets-datacollector-user-libs:Z
      - ./plugins/streamsets/libs-extras/streamsets-datacollector-jdbc-lib/ojdbc8.jar:/opt/streamsets-datacollector-3.22.2/streamsets-libs-extras/streamsets-datacollector-jdbc-lib/lib/ojdbc8.jar:Z
      - ./plugins/streamsets/libs-extras/streamsets-datacollector-jdbc-lib/mssql-jdbc-10.2.0.jre8.jar:/opt/streamsets-datacollector-3.22.2/streamsets-libs-extras/streamsets-datacollector-jdbc-lib/lib/mssql-jdbc-10.2.0.jre8.jar:Z
    ulimits:
      nofile:
        soft: 32768
        hard: 32768
    user: '1000'
    command:
      - dc
      - -exec
      - -verbose
    entrypoint:
      - /pre-docker-entrypoint.sh
    restart: unless-stopped
  #  ================================== Zeppelin ========================================== #
  zeppelin:
    image: trivadis/apache-zeppelin:0.10.0-spark3.2.1-hadoop3.2
    container_name: zeppelin
    hostname: zeppelin
    labels:
      com.platys.name: zeppelin
      com.platys.webui.title: Apache Zeppelin UI
      com.platys.webui.url: http://${PUBLIC_IP}:28080
    ports:
      - 28080:8080
      - 6060:6060
      - 5050:5050
      - 4050-4054:4050-4054
    env_file:
      - ./conf/hadoop.env
    environment:
      CORE_CONF_fs_s3a_endpoint: http://minio-1:9000
      CORE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_endpoint: http://minio-1:9000
      HIVE_SITE_CONF_fs_s3a_access_key: V42FCGRVMK24JJ8DHUYG
      HIVE_SITE_CONF_fs_s3a_secret_key: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      HIVE_SITE_CONF_fs_s3a_path_style_access: 'true'
      HIVE_SITE_CONF_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_impl: org.apache.hadoop.fs.s3a.S3AFileSystem
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_endpoint: http://minio-1:9000
      SPARK_DEFAULTS_CONF_spark_hadoop_fs_s3a_path_style_access: 'true'
      SPARK_HADOOP_FS_S3A_ACCESS_KEY: V42FCGRVMK24JJ8DHUYG
      SPARK_HADOOP_FS_S3A_SECRET_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      # for awscli & s3cmd
      AWS_ACCESS_KEY_ID: V42FCGRVMK24JJ8DHUYG
      AWS_SECRET_ACCESS_KEY: ${PLATYS_AWS_SECRET_ACCESS_KEY:-bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza}
      AWS_ENDPOINT: http://minio-1:9000
      AWS_DEFAULT_REGION: us-east-1
      SPARK_DEFAULTS_CONF_spark_jars_repositories:
      SPARK_DEFAULTS_CONF_spark_jars_packages:
      SPARK_DEFAULTS_CONF_spark_jars_excludes:
      SPARK_DEFAULTS_CONF_spark_jars: /extra-jars/postgresql-42.3.4.jar
      SPARK_DEFAULTS_CONF_spark_jars_ivySettings:
      SPARK_DEFAULTS_CONF_spark_sql_catalogImplementation: hive
      CORE_CONF_fs_defaultFS: s3a://admin-bucket
      SPARK_DEFAULTS_CONF_spark_sql_warehouse_dir: s3a://admin-bucket/hive/warehouse
      SPARK_DEFAULTS_CONF_spark_yarn_dist_files: /spark/conf/hive-site.xml
      SPARK_DEFAULTS_CONF_spark_driver_extraJavaOptions:
      SPARK_DEFAULTS_CONF_spark_executor_extraJavaOptions:
      ZEPPELIN_ADDR: 0.0.0.0
      ZEPPELIN_PORT: '8080'
      ZEPPELIN_MEM: -Xms1024m -Xmx1024m -XX:MaxMetaspaceSize=512m
      ZEPPELIN_INTERPRETER_CONNECT_TIMEOUT: 120000
      ZEPPELIN_INTERPRETER_DEP_MVNREPO: https://repo.maven.apache.org/maven2
      ZEPPELIN_ADMIN_USERNAME: admin
      ZEPPELIN_ADMIN_PASSWORD: changeme
      ZEPPELIN_USER_USERNAME: zeppelin
      ZEPPELIN_USER_PASSWORD: changeme
      # set spark-master for Zeppelin interpreter
      ZEPPELIN_SPARK_MASTER: spark://spark-master:7077
      ZEPPELIN_NOTEBOOK_DIR: notebook
      ZEPPELIN_NOTEBOOK_CRON_ENABLE: 'True'
      PYSPARK_PYTHON: python3
      SPARK_SUBMIT_OPTIONS: ' --conf spark.ui.port=4050 --conf spark.driver.host=zeppelin --conf spark.driver.port=5050 --conf spark.driver.bindAddress=0.0.0.0 --conf spark.blockManager.port=6060 --conf spark.driver.extraJavaOptions=-Dcom.amazonaws.services.s3.enableV4 --conf spark.executor.extraJavaOptions=-Dcom.amazonaws.services.s3.enableV4'
    volumes:
      - ./data-transfer:/data-transfer
      - ./plugins/spark/jars:/extra-jars
      - ./container-volume/spark/logs/:/var/log/spark/logs
      - ./conf/s3cfg:/root/.s3cfg.template
    restart: unless-stopped
  #  ================================== Oracle XE ========================================== #
  oracledb-xe:
    image: gvenzl/oracle-xe:21.3.0-full
    container_name: oracledb-xe
    hostname: oracledb-xe
    labels:
      com.platys.name: oracledb-xe
    ports:
      - 1522:1521
    environment:
      ORACLE_PASSWORD: EAo4KsTfRR
      ORACLE_CHARACTERSET: AL32UTF8
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/oraclexe:/container-entrypoint-initdb.d/
    restart: unless-stopped
  #  ================================== Oracle SQLcl ========================================== #
  oracle-sqlcl:
    image: trivadis/oracle-sqlcl:latest
    container_name: oracle-sqlcl
    hostname: oracle-sqlcl
    labels:
      com.platys.name: oracle-sqlcl
    environment:
      ORACLE_HOST: oracledb-xe
      ORACLE_PORT: 1521
      ORACLE_SERVICE: XEPDB1
    volumes:
      - ./data-transfer:/data-transfer
    command: tail -f /dev/null
    restart: unless-stopped
  #  ================================== SQL Server ========================================== #
  sqlserver:
    image: mcr.microsoft.com/mssql/server:2019-latest
    hostname: sqlserver
    container_name: sqlserver
    labels:
      com.platys.name: sqlserver
    ports:
      - 1433:1433
    environment:
      ACCEPT_EULA: Y
      SA_PASSWORD: abc123abc123!
      MSSQL_PID: Express
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== PostgreSQL ========================================== #
  postgresql:
    image: postgres:13
    container_name: postgresql
    hostname: postgresql
    labels:
      com.platys.name: postgresql
    ports:
      - 5432:5432
    environment:
      - POSTGRES_PASSWORD=abc123!
      - POSTGRES_USER=postgres
      - POSTGRES_DB=postgres
      - POSTGRES_MULTIPLE_DATABASES=demodb
      - POSTGRES_MULTIPLE_USERS=demo
      - POSTGRES_MULTIPLE_PASSWORDS=abc123!
      - PGDATA=/var/lib/postgresql/data/pgdata
      - DB_SCHEMA=demo
    volumes:
      - ./data-transfer:/data-transfer
      - ./init/postgresql:/docker-entrypoint-initdb.d/
    restart: unless-stopped
  #  ================================== Adminer ========================================== #
  adminer:
    image: adminer:latest
    container_name: adminer
    hostname: adminer
    labels:
      com.platys.name: adminer
      com.platys.webui.title: Adminer UI
      com.platys.webui.url: http://${PUBLIC_IP}:28131
    ports:
      - 28131:8080
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== Trino ========================================== #
  trino-1:
    image: trinodb/trino:378
    hostname: trino-1
    container_name: trino-1
    labels:
      com.platys.name: trino
      com.platys.webui.title: Trino UI
      com.platys.webui.url: http://${PUBLIC_IP}:28082
    ports:
      - 28082:8080
    environment:
      S3_ENDPOINT: http://minio-1:9000
      S3_AWS_ACCESS_KEY: V42FCGRVMK24JJ8DHUYG
      S3_AWS_SECRET_KEY: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      S3_PATH_STYLE_ACCESS: 'true'
      POSTGRESQL_USER: postgres
      POSTGRESQL_PASSWORD: abc123!
      ORACLE_USER: PRODUCT
      ORACLE_PASSWORD: abc
      # this is only generated to keep the structure valid if no other env variables are present
      IGNORE: ignore
    volumes:
      - ./data-transfer:/data-transfer
      - ./conf/trino/single/config.properties:/etc/trino/config.properties
      - ./conf/trino/single/node.properties:/etc/trino/node.properties
      - ./conf/trino/catalog/minio.properties:/etc/trino/catalog/minio.properties
      - ./conf/trino/catalog/postgresql.properties:/etc/trino/catalog/postgresql.properties
      - ./conf/trino/catalog/oracle.properties:/etc/trino/catalog/oracle.properties
    restart: unless-stopped
  trino-cli:
    image: trivadis/trino-cli:latest
    hostname: trino-cli
    container_name: trino-cli
    labels:
      com.platys.name: trino
    volumes:
      - ./data-transfer:/data-transfer
    tty: true
    restart: unless-stopped
  #  ================================== Presto ========================================== #
  dremio-1:
    image: dremio/dremio-oss:20.1
    container_name: dremio-1
    hostname: dremio-1
    labels:
      com.platys.name: dremio
      com.platys.webui.title: Dremio UI
      com.platys.webui.url: http://${PUBLIC_IP}:9047
    ports:
      - 9047:9047
      - 31010:31010
      - 45678:45678
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== Minio ========================================== #
  minio-1:
    image: minio/minio:RELEASE.2022-04-01T03-41-39Z
    container_name: minio-1
    hostname: minio-1
    labels:
      com.platys.name: minio
      com.platys.webui.title: MinIO UI
      com.platys.webui.url: http://${PUBLIC_IP}:9000
    ports:
      - 9000:9000
      - 9010:9010
    environment:
      MINIO_ROOT_USER: V42FCGRVMK24JJ8DHUYG
      MINIO_ROOT_PASSWORD: bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
      MINIO_REGION_NAME: us-east-1
      MINIO_BROWSER: 'True'
      #MINIO_PUBLIC_IPS: minio-1
      #MINIO_DOMAIN: minio.io
      #MINIO_DEFAULT_BUCKETS: 'admin-bucket,'
    volumes:
      - ./data-transfer:/data-transfer
    command: server /data --console-address ":9010"
    restart: unless-stopped
  #  ================================== Minio MC ========================================== #
  minio-mc:
    image: minio/mc:latest
    container_name: minio-mc
    hostname: minio-mc
    labels:
      com.platys.name: minio
    volumes:
      - ./data-transfer:/data-transfer
#      - ./conf/minio/config.json:/root/.mc/config.json
    entrypoint:
      - /bin/sh
      - -c
      - |
        sleep 10
        mc alias set minio-1 http://minio-1:9000 V42FCGRVMK24JJ8DHUYG bKhWxVF3kQoLY9kFmt91l+tDrEoZjqnWXzY9Eza
        mc mb --ignore-existing minio-1/admin-bucket
           for i in $$(echo "" | sed "s/,/ /g")
        do
          mc mb --ignore-existing minio-1/$$i
        done
        #
        while [ 1 -eq 1 ];do sleep 60;done
    restart: unless-stopped
  #  ================================== MockServer ========================================== #
  mockserver:
    image: mockserver/mockserver:latest
    container_name: mockserver
    hostname: mockserver
    labels:
      com.platys.name: mockserver
      com.platys.webui.title: MockServer UI
      com.platys.webui.url: http://${PUBLIC_IP}:28273/mockserver/dashboard
    ports:
      - 28273:1090
    environment:
      MOCKSERVER_SERVER_PORT: 1090
      MOCKSERVER_LOG_LEVEL: DEBUG
    volumes:
      - ./data-transfer:/data-transfer
      - /var/run/docker.sock:/var/run/docker.sock
    command: --admin-password '$$2y$$05$$NrPTXkUOIHTTbdHUqdAZVuSbncaZ9frWZYXDbA4v/WYqY0nAY1Sui'
    restart: unless-stopped
  #  ================================== File Browser ================= #
  file-browser:
    image: filebrowser/filebrowser:latest
    container_name: file-browser
    hostname: file-browser
    labels:
      com.platys.name: file-browser
      com.platys.webui.title: File Browser UI
      com.platys.webui.url: http://${PUBLIC_IP}:28178
    ports:
      - 28178:80
    volumes:
      - ./data-transfer:/srv
      - ./conf/filebrowser/filebrowser.json:/.filebrowser.json
    restart: unless-stopped
  #  ================================== Wetty ========================================== #
  wetty:
    image: svenihoney/wetty:latest
    container_name: wetty
    hostname: wetty
    labels:
      com.platys.name: wetty
      com.platys.webui.title: WeTTY UI
      com.platys.webui.url: http://${PUBLIC_IP}:3001
    ports:
      - 3001:3000
    environment:
      - REMOTE_SSH_SERVER=${DOCKER_HOST_IP}
      - REMOTE_SSH_PORT=22
      - REMOTE_SSH_USER=
      - WETTY_PORT=3000
    volumes:
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  #  ================================== markdown-viewer ========================================== #
  markdown-viewer:
    image: trivadis/markdown-web:latest
    container_name: markdown-viewer
    hostname: markdown-viewer
    labels:
      com.platys.name: markdown-viewer
      com.platys.webui.title: Markdown Viewer UI
      com.platys.webui.url: http://${PUBLIC_IP}:80
    ports:
      - 80:80
    volumes:
      - ./artefacts:/home/python/markdown
      - ./data-transfer:/data-transfer
    restart: unless-stopped
  markdown-renderer:
    image: trivadis/jinja2-renderer:latest
    container_name: markdown-renderer
    hostname: markdown-renderer
    labels:
      com.platys.name: markdown-renderer
    environment:
      USE_PUBLIC_IP: 'True'
      PUBLIC_IP: ${PUBLIC_IP}
      DOCKER_HOST_IP: ${DOCKER_HOST_IP}
      DATAPLATFORM_HOME: ${DATAPLATFORM_HOME}
      PLATYS_PLATFORM_NAME: platys-platform
      PLATYS_PLATFORM_STACK: trivadis/platys-modern-data-platform
      PLATYS_PLATFORM_STACK_VERSION: 1.15.0-preview
      PLATYS_COPY_COOKBOOK_DATA: 'True'
    volumes:
      - ./artefacts/templates:/templates
      - ./artefacts/templates:/scripts
      - .:/variables
      - ./artefacts:/output
      - ./data-transfer:/data-transfer
  #  ================================== data-provisioning ========================================== #
  data-provisioning:
    image: trivadis/platys-modern-data-platform-data:1.15.0-preview
    container_name: data-provisioning
    hostname: data-provisioning
    labels:
      com.platys.name: data-provisioning
    volumes:
      - ./data-transfer:/data-transfer
volumes:
  data-transfer-vol:
    name: data_transfer_vol
