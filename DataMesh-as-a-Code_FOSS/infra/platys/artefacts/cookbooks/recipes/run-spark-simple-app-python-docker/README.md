# Run Python Spark Application using Docker

This recipe will show how you can build a Docker image with your Python Spark Application, which submits the application against the Spark Cluster of the dataplatform when running the container. 

* Platform services needed: `SPARK`

## Building the Container with the Spark Application

The Spark application we are going to use is available in this [GitHub project](https://github.com/TrivadisPF/spark-simple-app). It already contains the following Dockerfile for creating the container:

```docker
FROM trivadis/apache-spark-python-template:2.4.7-hadoop2.8

MAINTAINER You <you@example.org>

ENV SPARK_APPLICATION_PYTHON_LOCATION /app/simple-app.py
# ENV SPARK_APPLICATION_ARGS "foo bar baz"
ENV SPARK_MASTER_NAME spark-master
ENV SPARK_MASTER_PORT 7077
```

For building a docker image locally, we first have to clone the project from GitHub:

```bash
git clone https://github.com/TrivadisPF/spark-simple-app.git
```

Navigate into the newly created folder

```
cd spark-simple-app/python
```

and build the docker image using the following command (replace `<repository>` by your own):

```bash
docker build -t <repository>/spark-simple-app-python .
```

When the build statement finishes, you should see the following output:

```
Successfully built d6c75bb0796b
Successfully tagged <repository>/spark-simple-app-python:latest
```

You can also push the application to Docker Hub to make it available or use the Docker Hub automatic builds to build the Docker image automatically.

## Run the Spark Application

With the Docker image in place, we can run the Spark application using the following command. 

```
docker run -ti --rm -e ENABLE_INIT_DAEMON=false -e CORE_CONF_fs_defaultFS=file:///tmp --network <network-name> <repository/spark-simple-app-python
```

Replace`<repository>` with the one you used when creating the docker image and `<network-name>` with the name of the network the dataplatform is running with. You can list the networks currently in use with `docker network list`. The network usually uses the name of the folder where the `docker-compose.yml` resides.

## Optionally: Add the Spark Application to the platform

You can also add the spark application as a service to the dataplatform. For that you should use the `docker-compose.override.yml` and not change the `docker-compose.yml` itself, which is generated by `platys` and would be overwritten the next time you generate the platform. 


```
version: "3.0"

services:
  spark-simple-app-python:
    image: <repository>/spark-simple-app-pyton:latest
    container_name: spark-simple-app-python
    hostname: spark-simple-app-python
  environment:
    ENABLE_INIT_DAEMON: false
    CORE_CONF_fs_defaultFS: file:///tmp
```

When you now do a new `docker-compose up -d` the `spark-simple-app-python` service will get started, which in turn submits the Spark application against the Spark cluster.